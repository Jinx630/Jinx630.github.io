<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN" "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<!-- saved from url=(0025)https://xh-liu.github.io/ -->
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="zh-CN"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta name="keywords" content="Xiangyu Wu, 武祥宇">
<meta name="description" content="Personal page of Xiangyu Wu at NJUST">

<link rel="stylesheet" href="./Homepage_files/jemdoc.css" type="text/css">
<title>Xiangyu Wu's Homepage </title>
</head>
<body data-new-gr-c-s-check-loaded="14.984.0" data-gr-ext-installed="">
<div id="layout-content">
	<div id="toptitle">
		<h1>Xiangyu Wu </h1>
	</div>
	<table class="imgtable">
		<tbody>
			<tr>
				<td>
					<img src="./Homepage_files/IMG_4034.JPG" alt="Xiangyu Wu" width="200px" height="200px">&nbsp;
				</td>
				<td align="left">
					<p>
						Ph.D. student<br> 
						<a href="http://home.njustkmg.cn:4056/" target="_blank">Knowledge Mining Group</a><br>
						<a href="https://www.njust.edu.cn/" target="_blank">Nanjing University of Science and Technology</a><br>
						836178735@qq.com <br>
						wxy_yyjhl@njust.edu.cn <br> <br>
						<a href="https://scholar.google.com.hk/citations?hl=zh-CN&user=R0GjVWIAAAAJ&view_op=list_works&sortby=pubdate" target="_blank">Google Scholar</a>&nbsp;&nbsp;&nbsp;<a href="" target="_blank">CV</a>
					</p>
				</td>
			</tr>
		</tbody>
	</table>

	<h2>About me</h2>
	<p id='bio'>I am currently a third-year Ph.D. student in <a href="http://home.njustkmg.cn:4056/" target="_blank">Knowledge Mining Group</a>, Nanjing University of Science and Technology. <br> I am supervised by <a href="https://gsmis.njust.edu.cn/open/TutorInfo.aspx?dsbh=4fwiNGyeUG2muTVlUu8gNQ==&yxsh=z70ppxVSQAs=&zydm=SwsWR9zpmmw=" target="_blank">Prof. Jianfeng Lu</a> and <a href="https://scholar.google.com.hk/citations?hl=zh-CN&user=_6NJip0AAAAJ" target="_blank">Prof. Yang Yang</a>. <br> My current research interests are MLLM, Text Generation, Test-Time Adaptation and Prompt Learning.</p>

	<h2>News</h2>
	<ul>
		<li><p>[04/2025] <b class="underlined">Two papers</b> was submitted to ACM MM 2025.</p></li>
		<li><p>[01/2025] <b class="underlined">One paper</b> accepted by ICLR 2025.</p></li>
		<li><p>[08/2024] <b class="underlined">One paper</b> accepted by ACML 2024.</p></li>
		<li><p>[04/2024] <b class="underlined">One paper</b> accepted by IJCAI 2024.</p></li>
		<li><p>[03/2024] <b class="underlined">One paper</b> accepted by ICME 2024.</p></li>
		<li><p>[06/2022] <b class="underlined">One paper</b> accepted by ICIP 2022.</p></li>
	</ul>
	  
	<h2>Research Intern</h2>
	<ul>
		<li>
			<p>Alibaba International Digital Commerce Group. (2023.08 to Now) <br> Working on - Algorithms - Visual & Multimodal.</p>
		</li>
	</ul>

	<h2>Publications</h2>
	<ul>
		<li>
			<p>
				<a href="https://arxiv.org/abs/2502.03777" target="_blank">
				Multi-Label Test-Time Adaptation with Bound Entropy Minimization
				</a> 
				[<a href="https://github.com/Jinx630/ML-TTA">Github</a>]
				<br>
				<b>Xiangyu Wu</b>, Feng Yu, Qing-Guo Chen, Yang Yang*, Jianfeng Lu*
				<br>
				<i>Proceedings of the 13th International Conference on Learning Representations (ICLR'2025)</i>
			</p>
		</li>
		<br>
		<li>
			<p>
				<a href="http://home.njustkmg.cn:4056/assets/pdf/publications/Conference%20Papers/Refining%20Visual%20Perception%20for%20Decoration%20Display%20A%20Self-Enhanced%20Deep%20Captioning%20Model.pdf" target="_blank">
				Refining Visual Perception for Decoration Display: A Self-Enhanced Deep Captioning Model
				</a> 
				[<a href="https://github.com/njustkmg/ACML-SET">Github</a>]
				<br>
				Longfei Huang, <b>Xiangyu Wu</b>, Jingyuan Wang, Weili Guo, Yang Yang*
				<br>
				<i>Proceedings of the 16th Asian Conference on Machine Learning (ACML'2024)</i>
			</p>
		</li>
		<br>
		<li>
			<p>
				<a href="https://arxiv.org/abs/2405.06926" target="_blank">
				TAI++: Text as Image for Multi-Label Image Classification by Co-Learning Transferable Prompt</a> 
				[<a href="https://github.com/Jinx630/Pseudo-Visual-Prompt">Github</a>]
				<br>
				<b>Xiangyu Wu</b>, Qingyuan Jiang, Yifeng Wu, Qingguo Chen, Yang Yang*, and Jianfeng Lu*
				<br>
				<i>Proceedings of the 33rd International Joint Conference on Artificial Intelligence (IJCAI'2024)</i>
			</p>
		</li>
		<br>
		<li>
			<p>
				<a href="http://home.njustkmg.cn:4056/assets/pdf/publications/Conference%20Papers/CoVLR_Coordinating_Cross-Modal_Consistency_and_Intra-Modal_Relations_for_Vision-Language_Retrieval.pdf" target="_blank">
				CoVLR: Coordinating Cross-Modal Consistency and Intra-Modal Relations for Vision-Language Retrieval</a> 
				[<a href="https://github.com/njustkmg/ICME2024-CoVLR">Github</a>]
				<br>
				Fengqiang Wan, <b>Xiangyu Wu</b>, Zhihao Guan, Yang Yang*
				<br>
				<i>IEEE International Conference on Multimedia and Expo (ICME'2024)</i>
			</p>
		</li>
		<br>
		<li>
			<p>
				<a href="https://trax-geometry.s3.amazonaws.com/cvpr_challenge/cvpr2021/recognition_challenge_technical_reports/3rd+Place+Solution+to+CVPR+2021+AliProducts+Challenge.pdf" target="_blank">
				QUES-TO-VISUAL GUIDED VISUAL QUESTION ANSWERING</a> 
				<br>
				<b>Xiangyu Wu</b>, Jianfeng Lu, Zhuanfeng Li, Fengchao Xiong
				<br>
				<i>IEEE International Conference on Image Processing (ICIP'2022)</i>
			</p>
		</li>
	</ul>

	<h2>Awards & Honors</h2>
	<ul>
		<li>
			<p>
				<span class="chinese"><b class="underlined">Excellence Award</b> WWW2025多模态对话系统意图识别挑战赛 <i>2025.01</i></span>
			</p>
		</li>
		<li>
			<p>
				<span class="chinese"><b class="underlined">First Place</b> 2024第三届 粤港澳大湾区国际算法算例大赛 多模态大模型学科能力综合强化 <i>2024.10</i></span>
			</p>
		</li>
		<li>
			<p>
				<span class="chinese"><b class="underlined">Second Prize</b> 2024全球人工智能技术创新大赛 赛道1 无人机视角下的双光目标检测 <i>2024.06</i></span>
			</p>
		</li>
		<li>
			<p>
				<span class="chinese"><b class="underlined">First Place</b> CVPR 2024 New Frontiers for Zero-shot lmage Captioning Evaluation <i>2024.03</i></span>
			</p>
		</li>
		<li>
			<p>
				<span class="chinese"><b class="underlined">Second Place</b> 2023第二届 粤港澳大湾区国际算法算例大赛 基于语言增强的图像新类别发现 <i>2024.01</i></span>
			</p>
		</li>
		<li>
			<p>
				<span class="chinese"><b class="underlined">First Place</b> ICCV 2023 The First Scientific Figure Captioning (SCICAP) Challenge <i>2023.10</i></span>
			</p>
		</li>
		<li>
			<p>
				<span class="chinese"><b class="underlined">First Place</b> ICCV 2023 Multi-modal Algorithmic Reasoning (SMART-101) Challenge <i>2023.10</i></span>
			</p>
		</li>
		<li>
			<p>
				<span class="chinese"><b class="underlined">First Place</b> 2023全球人工智能技术创新大赛 赛道1 影像学NLP-医学影像诊断报告生成 <i>2023.06</i></span>
			</p>
		</li>
		<li>
			<p>
				<span class="chinese"><b class="underlined">First Place</b> CVPR 2023 foundation model challenge Cross-Modal Image Retrieval <i>2023.05</i></span>
			</p>
		</li>
		<li>
			<p>
				<span class="chinese"><b class="underlined">First Place</b> CVPR 2023 New Frontiers for Zero-shot lmage Captioning Evaluation <i>2023.05</i></span>
			</p>
		</li>
		<li>
			<p>
				<span class="chinese"><b class="underlined">Second Place</b> WSDM 2023 Cup: Visual Question Answering Challenge <i>2023.01</i>
			</p>
		</li>
	</ul>

	<h2>Academic Services</h2>
	<ul>
		<li>
			<p>Journal Reviewer of PR</p>
		</li>
		<li>
			<p>Conference Reviewer of AAAI, ICLR</p>
		</li>
	</ul>
</div>

</body>
</html>
